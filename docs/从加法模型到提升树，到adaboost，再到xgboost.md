---

layout: post

titile: 从加法模型到提升树，到adaboost，再到xgboost

date: 2019-03-25 16:42:00.000000000 +09:00

---

# 从加法模型到提升树，到adaboost，再到xgboost 

提升方法的思路是虽然一个分类器效果可能不好，但多个分类器组合可能会比较好。使用加法模型来实现这个思路： 

$$ f(x) = \sum_{m=1}^M\beta_mG_m(x) $$

其中M表示分类器的个数，$\beta$表示分类器的权重，$G(x)$表示单个分类器。现在的目标就是找出一系列参数使得最后模型的损失函数最小： 

$$arg \min\limits_{\beta,\gamma}L(y,\sum_{m=1}^{M}\beta_mG_m(x, \gamma_m))$$

其中$y$表示标记，$\gamma_m$表示第m个分类器的具体参数。具体的做法可以是确定了分类器的具体个数，然后再使用一次计算求出这些参数，这样显然比较笨拙，与直接使用一个分类器进行学习没什么区别。还有一种做法是先学一个分类器，查看效果（损失），然后在这个基础之上进行下一个分类器的学习，不断地学习与修正使得效果不断提升（损失减小），这样的思路更符合正常逻辑，这就是前向分步算法。 

$$arg \min\limits_{\beta_m\gamma_m}L(y, f_{m-1}+\beta_mG_m(x, \gamma_m))$$ 

每一步都只是计算一个$\beta, \gamma$，并且计算完毕后能够知道损失是否在下降，是否下降的够快，下一个模型可以针对剩下的损失进行优化。 

## 加法模型与adaboost 

adaboost 是损失函数为指数损失时的一个加法模型的特例。指数损失： 

$$L(y, f(x)) = exp[-yf(x)]$$

代入加法模型： 

$$arg \min\limits_{\beta_m,\gamma_m}\sum_{i=1}^{N}exp(-y_i(\sum_{m=1}^M\beta_mG_m(x_i, \gamma_m)))$$

其中i表示样本标识，N表示样本个数。对于第M个分类器来说，前M-1个分类器是已知的。 

$$\begin{aligned}&arg\min\limits_{\beta_M,\gamma_M}\sum_{i=1}^{N}exp(-y_i(\sum_{m=1}^{M-1}\beta_mG_m(x_i, \gamma_m)+\beta_MG_M(x_i, \gamma_M)))\\&=arg \min\limits_{\beta_M,\gamma_M}\sum_{i=1}^{N}w_iexp(-y_i\beta_MG_M(x_i, \gamma_M))\\&w_i = exp(-y_i(\sum_{m=1}^{M-1}\beta_mG_m(x_i, \gamma_m))\end{aligned}$$

在这个问题中涉及两个部分的计算，一是模型中的$\gamma_M$，另一个是模型前的系数$\beta_M$。对于第一个问题，adaboost是一个二分类树，具体可以使用树模型进行学习。那么学习后的最终结果可以表达为： 

$$\begin{aligned}&arg \min\limits_{\beta_M}\sum_{i=1}^N w_iexp(-y_i\beta_MG_M(x_i))\\=&arg \min\limits_{\beta_M}[\sum_{y_i=G_M(x_i)}w_iexp(-\beta)+\sum_{y_i\ne G_M(x_i)}w_iexp(\beta)]\end{aligned}$$

上式是针对两种情况分开进行表达，一种是模型预测正确，另一种是模型预测错误。第二个问题是如何求解$\beta_M$，具体做法是对上式进行关于$\beta_M$求导，然后令其等于零进行求解。由于上式两项的求和条件不一致，即使求导也不能合并进行求解，先使用一个技巧将两项的求和条件调整为一致。 

$$\begin{aligned}\sum_{y_i = G_M(x_i)}w_iexp(-\beta) &= \sum_{i=1}^{N}w_iexp(-\beta)I(y_i=G_M(xi))\\\sum_{y_i \ne G_M(x_i)}w_iexp(\beta) &= \sum_{i=1}^{N}w_iexp(\beta)I(y_i\ne G_M(xi))\\I(y_i=G_M(x_i)) &= 1- I(y_i\ne G_M(x_i))\\I(y_i\ne G_M(x_i)) &= 1 - I(y_i = G_M(x_i))\\\end{aligned}$$

根据上式可以将待求解式子调整并求导得到： 

$$\begin{aligned}&(exp(\beta)+exp(-\beta))\sum_{i=1}^Nw_iI(y_i\ne G_M(x_i)) - \sum_{i=1}^Nw_iexp(-beta) = 0\\ &\frac{\sum_{i=1}^{N}w_iI(y_i\ne G_M(x_i))}{\sum_{i=1}^Nw_i} = \sum_{i=1}^Nw_iexp(-\beta) \end{aligned} $$

这里选择的是$I(y_i\ne G_M(x_i))$，那么$e_M = \frac{\sum_{i=1}^{N}w_iI(y_i\ne G_M(x_i))}{\sum_{i=1}^Nw_i}$对应的就是带权重的错误率。如果选择$I(y_i=G_M(x_i))$，相应的就会得到一个正确率的计算版本。通过变换并求自然对数可以得到$\beta$的表达式： 

$$\begin{aligned} [exp(\beta)+exp(-\beta)]e_M &= exp(-\beta)\\ (1-e_M)exp(-\beta) &= exp(\beta)e_M\\\log(1-e_M) - \beta &= \beta+ \log e_M\\ \beta = &\frac{1}{2}\log{\frac{1-e_M}{e_M}} \end{aligned} $$

在计算过程中还有一个问题需要关注，那就是$w_i$，它是样本的权重，初始化时各个样本的权重是一致的，都是1，在后续的过程中，样本的权重是通过下式进行计算更新的： 

$$w_i= exp(-y_i(\sum_{m=1}^{M-1}\beta_mG_m(x_i, \gamma_m))$$

也就是在生成第M棵树时，使用的样本权重是前面M-1棵树调整的结果。并且后续的权重调整原有的基础上乘以某一个倍数，这个倍数只与第M棵树相关。 

$$
exp(-y_i\beta_iG_M(x_i))=\begin{cases} 
exp(\beta_i) &(y_i \ne G_M(x_i))\\ 
exp(-\beta_i) &(y_i=G_M(x_i)) 
\end{cases}
$$

## 最速下降法到加法模型再到梯度提升 

《统计学习方法》方法中，对梯度提升的描述是“利用损失函数的负梯度在当前模型的值作为回归问题提升树算法中的残差的近似值，拟合一个回归树”。初一看到会觉得跳跃性很大，负梯度跟残差有什么关系？再仔细看书，这个结论是通过最速下降法推导得来的，那么最速下降法讲的是什么呢？ 

为了求得某个函数的极值（或者说求在哪个点上函数产生极值），使用函数的负梯度作为前进方向，使用线索搜的方式（抽象层面来说它是一类优化方法，具体来说它是一种技术）求得一个合适的步长使得函数最优化。他的核心可以表示为： 

$$
\begin{aligned} 
&x_{k+1} = x_{k} + \alpha_kd_k\\ 
&\alpha_k = arg \min\limits_{\alpha} f[x+\alpha d] 
\end{aligned} 
$$

其中x表示待优化自变量，$\alpha_k$代表第k次前进步长，$d_k$代表第k次前进方向，$d_k$一般取待优化函数的负梯度$-\nabla f$。 

最速下降法的更新公式可以很好的与加法模型联系起来： 

$$
\sum_{m=1}^M\beta_mG_m(x) = \sum_{m=1}^{M-1}\beta_mG_m(x) + \beta_{M-1}G_M(x) 
$$

它们的更新量两两对应，这也是《统计学习方法》中使用负梯度作为近似残差学习一个回归树的来源。但其使用“残差”一词描述该梯度是不准确的。残差描述的是当前模型预测与数据间的差异，实际上梯度提升在使用负梯度学习一颗回归树后还需要乘以一个步长才整合进模型，那么准确来说负梯度与步长的乘积才是拟合了残差。 

## 梯度提升与xgboost 

梯度提升与xgboost都是提升方法，都是以加法模型的前向分步算法为基础，所以xgboost也不能在树的这个粒度上进行并行，也必须是先学一个树，然后再进行下一个树的学习。梯度提升与xgboost有什么区别？大部分人都会说梯度提升只用到了一阶导数，而xgboost用到了二阶导数。追根溯源要从泰勒展开说起，不管是梯度提升还是xgboost，目标都是使得损失函数最小化，对损失函数泰勒展开到二阶导项： 

$$
\sum_{i=1}^N L[y_i, f_{m-1}(x_i)+G_M(x_i)] = \sum_{i=1}^NL[y_i,f_{m-1}(x_i)] + \sum_{i=1}^N\frac{\partial L[y_i, f_{m-1}(x_i)]}{\partial f_{m-1}(x_i)}G_M(x_i) + \sum_{i=1}^N\frac{1}{2} \frac {\partial ^2L[y_i, f_{m-1}(x_i)]}{\partial^2f_{m-1}(x_i)}G_{M}^2(x_i) + O(f_{m-1}^2) 
$$

如果只是用到一阶导数的话，那么就是梯度提升，这个和之前写的有什么联系呢？概括来讲，最速下降或者梯度下降法是通过泰勒展开将损失函数近似成一次函数进行优化的过程。xgboost则是使用到了二阶导数，他的优化方法使用的是牛顿法。到这里基本就说清楚了他们根源上的的区别。按照xgboost的思路进行推导，令$g_i = \frac{\partial L[y_i, f_{m-1}(x_i)]}{\partial f_{m-1}(x_i)}, h_i = \frac {\partial ^2L[y_i, f_{m-1}(x_i)]}{\partial^2f_{m-1}(x_i)}$，省略掉高阶导项，上式可以简化为： 

$$
\sum_{i=1}^N L[y_i, f_{m-1}(x_i)+G_M(x_i)] \approx \sum_{i=1}^NL[y_i,f_{m-1}(x_i)] + \sum_{i=1}^Ng_iG_M(x_i) + \sum_{i=1}^N\frac{1}{2} h_iG_{M}^2(x_i) 
$$

求上式最小值，可以求关于$G_M(x_i)$导数并令其为零，化简得到： 

$$
G_M(x) = -\frac{\sum_{i=1}^{N}g_i}{\sum_{i=1}^{N}h_i} 
$$

上式即为树的生成公式。 

## xgboost的细节 

xgboost准确来讲是一个提升方法的工具包，它在上述理论基础上进行很多的优化 

### 防止过拟合 

xgboost论文中同时使用树节点数和L2正则项进行推导，它们表示为： 

$$
\Omega(G_M(x)) = \gamma T + \frac{1}{2} \lambda\sum_{j=1}^{T} w_j^2 
$$

其中T表示为叶子节点个数，$w_j$表示第j个叶子节点的权重。 

在正则化方面实际上可以使用其他不同的方式，提升方法的正则化可以分为三个不同的类别。第一种是在提升过程的正则化，比如xgboost的shrinkage，以及指定树的棵数。第二种是针对树本身，如限制叶子节点个数，学习时每个叶子节点包含的样本数，L1和L2正则化等。第三种是随机策略，包括行随机采样（样本随机采样）、列随机采样（特征随机采样）。 

### 近似算法 

近似算法是要解决贪婪算法效率不高的问题。xgboost寻找树的分割点如果使用贪婪算法，需要针对每个特征的每一个值进行枚举计算Gain来判断最佳分割点，对于数据量较大的情况是效率极低的。近似算法根据每个特征值的分位数进行分桶，然后计算这些分位数对应的Gain值。这里有个方便之处是桶里的g值h值之和是可以提前计算好并且重复利用的。论文中提到近似算法也分全局和局部，就是在什么阶段选择分位数，全局就是在生成树之前选定好分位数，而局部则是对已经生成的某个节点上的样本进行分位数的选定。 

### 缺失值的处理 

xgboost针对缺失值的处理提出了个“新”的算法。一个节点的分裂方向有两个。如果样本的特征值缺失，则统一划归到一个方向，这个方向的选择是通过分别试算缺失样本在左侧和右侧的的Gain值来进行决定的，哪个方向的Gain值大就选择哪个方向。 

### 并行 

前面提到xgboost同样无法在树的粒度上进行并行，他的并行策略使用在特征维度。在使用并行运算前对特征值进行了排序，然后使用CSC压缩存储，每一个特征存一个block。那么对应的样本标签根据索引取得（我分析就是一个指针），这样又引出了另一个问题，就是取索引不连续而数据量大又无法将标签数据全部存入内存，导致IO堵塞产生效率下降问题。论文提出了一个Cache-aware 算法，具体怎么实现的没搞懂，大意是单独弄一个线程进行数据的获取，解决IO堵塞。解决这些问题后，要选择分裂点，就可以所有特征同时进行近似算法的计算，因为特征值是预先就已经排好序了，所以减少了很多的重复计算，提高了效率。 

::: slot reference
1. liuxi
2. liuxi
3. liuxi
:::
