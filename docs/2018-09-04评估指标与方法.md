---
layout: post
titile: 评估指标与方法
date: 2018-09-04 16:42:00.000000000 +09:00
---

# 评估指标与方法

## 有哪些评估指标
mean sqaure error、错误率、精度、查准率、查全率、F1、P-R、ROC、AUC
### 回归问题的评估
一般使用的是mean square error。
$$
E(f;D) = \frac{1}{m}\sum_{i=1}^m(f(x_i)-y_i)^2
$$
其中D表示样本空间，m为样本个数，f是模型，$x_i$是某个样本，$y_i$是对应的标签数值。
在数据科学比赛中可能会根据问题提出新的评估指标。
### 分类问题的评估
主要讲二分类，多分类情况可以以二分类进行推广。
错误率与精度容易理解，他们是模型预测错误与正确的百分比。他们的和是1。
需要提到的一个重要概念就是混淆矩阵，它不光叫混淆矩阵，也让我混淆和很长时间。
| 实际结果<br>预测结果|正  |负  |
| --- | --- | --- |
| 正 | TP | FN |
| 负 | FP | TP |
这四个标志面向的是预测结果。比如TP，TRUE POSITVE，他指的是模型预测的POSITIVE结果是正确的。抓好这个原则，就不会搞混淆这个矩阵了。这个矩阵是基础，后面的指标都与这个矩阵相关。
查准率P与查全率R面向的是正分类。
$$
\begin{aligned}
P &= \frac {TP} {TP+FP}\\ R &= \frac{TP}{TP+FN}
\end{aligned}
$$
查准率的含义是，在所有预测的正样本中有多少比例被预测正确。查全率的含义是，在所有正样本中有多少比例被准确预测出来。
分类问题事实上评估的是概率排序质量。在二分类中往往以概率值0.5为阈值进行正负样本划分，而实际问题中可以调整阈值进行样本标签的预测，为了不局限于0.5阈值，考察样本根据概率值排序的质量来评估模型的好坏。这样就出现了P-R曲线，每个样本经过模型得到一个概率，根据这个概率值排好序后，依次将每个样本划分为正样本计算P、R值得到P-R曲线。《机器学习》中的P-R曲线示意图存在误导，在两个端点时（1，0），（0，1），通过公式可以知道这两个点应该是取不到的。如果一条P-R曲线包含另一条，则前者模型的性能强于后者，但对于交叉的曲线不好判断，BEP是一种解决方案，也就是找到P=R的点，并比较这个值，这也好理解，查准率与查全率同时大的模型性能更强。
F1是另一种基于查全率与查准率的指标，它是查全率与查全率的调和平均。
$$
F1 = \frac {2} {\frac {1}{P} + \frac {1}{R}}
$$
只有在查全率与查准率都不低的情况下F1才会高。这个指标的使用是需要给定某个划分阈值的。
ROC曲线与P-R曲线类似，只是作图使用的评估指标不一样，它使用的是真正例率TPR与假正例率FPR
$$
\begin{aligned}
TPR = & \frac{TP}{TP + FN}\\
FPR = & \frac{FP}{TN + FP}
\end{aligned}
$$
TPR与查全率一致。FPR表达的含义是在真的反例中预测错的比例有多少。ROC的做法是将按照预测概率排序好的样本逐个预测为正例后计算相应的TPR与FPR然后作图。通过对两个指标含义的理解，我们希望的情况是TPR不断增大的同时FPR保持较慢的增长，这样理想的曲线会表现为一条上凸的曲线，而当模型没有预测能力时，也就是正负样本随机预测时，可以知道TPR与FPR几乎相等，也就是一条斜率为1的直线。如果ROC是一条下凹的曲线，说明FPR增长较快，应该考虑是否将标签设置反了。
AUC是area under curve的缩写。他的含义是ROC曲线下的面积表示模型的性能。按照我们期望的情况，当面积为1或者为0时肯定是最理想的状态，面积为0.5是最糟糕的情况。
AUC有两种解释，一是根据TPR、FPR画出ROC曲线然后计算曲线下的面积，二是AUC表示从样本中随机抽取一对正样本与负样本，正样本得分比负样本得分大的概率。这两个解释直观理解是没有什么联系的，但想想两个解释完全不相关的东西却描述的是同一件事，这中间肯定是有什么地方联系着的。基于这两个解释，计算AUC的方法也有两种。
AUC还有一个特性是对测试样本的类别比例不敏感。
## 哪些方法进行评估
留出法、留一法、交叉验证、自助法
评估方法的关键在于如何划分测试集的数据。留出法是在样本中分割出一部分作为测试集，要求是保持数据分布，一般采用随机采样，如果数据量不够大时，可以根据类别比例进行采样，也就是分层采样。交叉验证未解决的问题是留出法随机采样导致的偏差，具体做法是将样本等分为K份，尽量保持每个样本符合总体样本分布，然后使用其中的K-1份数据进行训练，剩下一份数据进行测试，然后对K个测试结果求平均，平衡了单次采样导致的偏差。自助法使用有放回的采样方法产生训练集与测试集，每一次有放回的采一个样本放入训练集，重复得到训练集，未在训练集中出现的样本作为测试集。可以证明样本中有36.8%的比例将不会进入训练集。一般常用的是留出法与交叉验证。
机器学习中还有一个词叫验证集，他的作用是用于模型调参。在模型训练过程中有很多的超参数需要去调整使得模型的泛化能力提升，验证集就是在调整完参数后对模型的泛化能力进行评估的数据。验证集往往与测试集容易混淆，他们两者的作用是不同的，验证集往往重复被使用，以使得模型在验证集上的效果达到最优，虽然验证集没有参与到模型训练的过程中，但是以验证集最优的目的进行的调参可能对未知数据效果很差，这时候就是用测试集来最终评定模型的泛化能力。所以测试集一定是不能作为重复使用的。虽然我们知道测试集的标签或者准确答案，我们也应该当做他是一份未知的数据来预测然后评估。在数据科学比赛中，我们提交的结果对应的就是测试集。而为了训练模型我们需要对提供的数据进行切分得到验证集。
## 过拟合、欠拟合与偏差、方差
什么是过拟合、欠拟合，偏差、方差分别代表什么，如何与过拟合、欠拟合联系
过拟合指的模型过度的拟合了训练集上的数据，而将训练集本身的一些数据特性学习了，这样在测试集上的效果就不会很好。欠拟合则是模型对训练集的特征未能完全学会，没有能力很好的预测测试集。在机器学习中一般讲bias-vias。对应的偏差与方差，偏差指的是期望输出与真实标记的差别：
$$
bias^2(x) = (\overline{f}(x)-y)^2
$$
其中$\overline{f}(x)$表示模型预测的期望值，$y$表示真是样本的标签。
方差指的是使用不同数据训练的模型预测结果与模型期望结果的偏差：
$$
var(x) = E_D[(f(x;D) - \overline{f}(x))^2]
$$
其中$f(x;D)$表示在D数据上学到的模型来预测x样本，$\overline{f}(x)$表示期望模型预测结果。
偏差描述的是一个模型对数据的拟合能力，方差表示的是不同数据训练出来的模型对预测结果的波动，也就是表示数据的波动会带模型预测的波动有多大。
噪声表示了数据的质量，是数据集中的标记关于真是标记的方差。如果噪声较大，数据质量较差，学到的模型也不会好。
$$
\varepsilon^2 = E_D[(y_D - y)^2]
$$
::: slot reference
1. 《机器学习》一书中对泛化误差描述为偏差、方差与噪声之和。
2. AUC计算方法总结：<https://www.cnblogs.com/peizhe123/p/5081559.html>
3. 机器学习中样本比例不平衡的处理方法：<http://alexkong.net/2013/06/introduction-to-auc-and-roc/>
4. 理解 ROC 和 AUC:<http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2015/11/20/understanding-ROC-and-AUC>
5. (Fawcett, 2006)，Fawcett, T. (2006). An introduction to ROC analysis. Pattern recognition letters, 27(8), 861-874.
:::